{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    " \n",
    "# Creating an instance\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    " \n",
    "# Logging into LinkedIn\n",
    "\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "\n",
    "time.sleep(5)\n",
    " \n",
    "\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "\n",
    "username.send_keys(\"insobyyman@gmail.com\")  # Enter Your Email Address\n",
    " \n",
    "\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "pword.send_keys(\"weedstream456123\")        # Enter Your Password\n",
    " \n",
    "\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    " \n",
    "# Opening Kunal's Profile\n",
    "# paste the URL of Kunal's profile here\n",
    "\n",
    "profile_url = \"https://www.linkedin.com/in/lunar-echo-78a918299/\"\n",
    " \n",
    "\n",
    "driver.get(profile_url)        # this will open the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    " \n",
    "# will be used in the while loop\n",
    "\n",
    "initialScroll = 0\n",
    "\n",
    "finalScroll = 1000\n",
    " \n",
    "\n",
    "while True:\n",
    "\n",
    "    driver.execute_script(f\"window.scrollTo({initialScroll},\n",
    "\n",
    "                                            {finalScroll})\n",
    "\n",
    "                          \")\n",
    "\n",
    "    # this command scrolls the window starting from\n",
    "\n",
    "    # the pixel value stored in the initialScroll \n",
    "\n",
    "    # variable to the pixel value stored at the\n",
    "\n",
    "    # finalScroll variable\n",
    "\n",
    "    initialScroll = finalScroll\n",
    "\n",
    "    finalScroll += 1000\n",
    " \n",
    "\n",
    "    # we will stop the script for 3 seconds so that \n",
    "\n",
    "    # the data can load\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # You can change it as per your needs and internet speed\n",
    " \n",
    "\n",
    "    end = time.time()\n",
    " \n",
    "\n",
    "    # We will scroll for 20 seconds.\n",
    "\n",
    "    # You can change it as per your needs and internet speed\n",
    "\n",
    "    if round(end - start) > 20:\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = driver.page_source\n",
    " \n",
    "# Now using beautiful soup\n",
    "\n",
    "soup = BeautifulSoup(src, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the HTML of the complete introduction box\n",
    "# that contains the name, company name, and the location\n",
    "\n",
    "intro = soup.find('div', {'class': 'pv-text-details__left-panel'})\n",
    " \n",
    "\n",
    "print(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of an error, try changing the tags used here.\n",
    " \n",
    "\n",
    "name_loc = intro.find(\"h1\")\n",
    " \n",
    "# Extracting the Name\n",
    "\n",
    "name = name_loc.get_text().strip()\n",
    "# strip() is used to remove any extra blank spaces\n",
    " \n",
    "\n",
    "works_at_loc = intro.find(\"div\", {'class': 'text-body-medium'})\n",
    " \n",
    "# this gives us the HTML of the tag in which the Company Name is present\n",
    "# Extracting the Company Name\n",
    "\n",
    "works_at = works_at_loc.get_text().strip()\n",
    " \n",
    " \n",
    "\n",
    "location_loc = intro.find_all(\"span\", {'class': 'text-body-small'})\n",
    " \n",
    "# Ectracting the Location\n",
    "# The 2nd element in the location_loc variable has the location\n",
    "\n",
    "location = location_loc[0].get_text().strip()\n",
    " \n",
    "\n",
    "print(\"Name -->\", name,\n",
    "\n",
    "      \"\\nWorks At -->\", works_at,\n",
    "\n",
    "      \"\\nLocation -->\", location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the HTML of the Experience section in the profile\n",
    "\n",
    "experience = soup.find(\"section\", {\"id\": \"experience-section\"}).find('ul')\n",
    " \n",
    "\n",
    "print(experience)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of an error, try changing the tags used here.\n",
    " \n",
    "\n",
    "li_tags = experience.find('div')\n",
    "\n",
    "a_tags = li_tags.find(\"a\")\n",
    "\n",
    "job_title = a_tags.find(\"h3\").get_text().strip()\n",
    " \n",
    "\n",
    "print(job_title)\n",
    " \n",
    "\n",
    "company_name = a_tags.find_all(\"p\")[1].get_text().strip()\n",
    "\n",
    "print(company_name)\n",
    " \n",
    "\n",
    "joining_date = a_tags.find_all(\"h4\")[0].find_all(\"span\")[1].get_text().strip()\n",
    " \n",
    "\n",
    "employment_duration = a_tags.find_all(\"h4\")[1].find_all(\n",
    "\n",
    "    \"span\")[1].get_text().strip()\n",
    " \n",
    "\n",
    "print(joining_date + \", \" + employment_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = driver.find_element(By.XPATH, \"//a[@data-link-to='jobs']/span\")\n",
    "# In case of an error, try changing the XPath.\n",
    " \n",
    "jobs.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_src = driver.page_source\n",
    " \n",
    "\n",
    "soup = BeautifulSoup(job_src, 'lxml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_html = soup.find_all('a', {'class': 'job-card-list__title'})\n",
    "# In case of an error, try changing the XPath.\n",
    " \n",
    "\n",
    "job_titles = []\n",
    " \n",
    "\n",
    "for title in jobs_html:\n",
    "\n",
    "    job_titles.append(title.text.strip())\n",
    " \n",
    "\n",
    "print(job_titles)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name_html = soup.find_all(\n",
    "\n",
    "  'div', {'class': 'job-card-container__company-name'})\n",
    "\n",
    "company_names = []\n",
    " \n",
    "\n",
    "for name in company_name_html:\n",
    "\n",
    "    company_names.append(name.text.strip())\n",
    " \n",
    "\n",
    "print(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   # for removing the extra blank spaces\n",
    " \n",
    "\n",
    "location_html = soup.find_all(\n",
    "\n",
    "    'ul', {'class': 'job-card-container__metadata-wrapper'})\n",
    " \n",
    "\n",
    "location_list = []\n",
    " \n",
    "\n",
    "for loc in location_html:\n",
    "\n",
    "    res = re.sub('\\n\\n +', ' ', loc.text.strip())\n",
    " \n",
    "\n",
    "    location_list.append(res)\n",
    " \n",
    "\n",
    "print(location_list)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
